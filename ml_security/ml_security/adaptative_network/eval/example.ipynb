{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS (Apple Silicon GPU)\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# I have a cnn model that I trained on the CIFAR10 dataset.\n",
    "# I want to see how robust it is to adversarial attacks.\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ml_security.adaptative_network.eval.utils import (\n",
    "    PreActBlock,\n",
    "    PreActResNet,\n",
    "    PreActResNetwithKAN,\n",
    "    CIFARCNNKAN,\n",
    "    CIFARCNN,\n",
    ")\n",
    "from ml_security.attacks.membership_inference_attack import create_attack_dataloader\n",
    "from ml_security.datasets.datasets import (\n",
    "    DATASET_REGISTRY,\n",
    "    DatasetType,\n",
    "    create_dataloader,\n",
    ")\n",
    "from ml_security.logger import logger\n",
    "from ml_security.utils.utils import get_device, set_seed\n",
    "\n",
    "# Set the seed\n",
    "set_seed(42)\n",
    "DEVICE = get_device()\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "\n",
    "model_path = \"cnn/CIFAR10/classic_cnn.pth\"\n",
    "\n",
    "# Load the model\n",
    "# model = PreActResNet(PreActBlock, [2, 2, 2, 2])\n",
    "model = CIFARCNN()\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "dataset = \"CIFAR10\"\n",
    "\n",
    "dataset = DatasetType[dataset]\n",
    "dataset_info = DATASET_REGISTRY[dataset]\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.247, 0.243, 0.261)),\n",
    "    ]\n",
    ")\n",
    "if dataset_info.origin == \"TORCHVISION\":\n",
    "    trainloader = create_dataloader(\n",
    "        dataset=dataset, batch_size=BATCH_SIZE, train=True, transformation=transform, max_samples=10000\n",
    "    )\n",
    "    valloader = create_dataloader(\n",
    "        dataset=dataset, batch_size=BATCH_SIZE, train=False, transformation=transform, max_samples=10000\n",
    "    )\n",
    "else:\n",
    "    raise ValueError(\"Unknown dataset origin.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "you can only change requires_grad flags of leaf variables.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 82\u001b[0m\n\u001b[1;32m     79\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m   \u001b[38;5;66;03m# Step size for each iteration\u001b[39;00m\n\u001b[1;32m     80\u001b[0m iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m40\u001b[39m     \u001b[38;5;66;03m# Number of iterations\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m \u001b[43mtest_l2_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 63\u001b[0m, in \u001b[0;36mtest_l2_attack\u001b[0;34m(model, test_loader, epsilon, alpha, iters)\u001b[0m\n\u001b[1;32m     60\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(DEVICE), target\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# Generate adversarial example\u001b[39;00m\n\u001b[0;32m---> 63\u001b[0m perturbed_data \u001b[38;5;241m=\u001b[39m \u001b[43ml2_pgd_attack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Re-classify the perturbed image\u001b[39;00m\n\u001b[1;32m     66\u001b[0m output \u001b[38;5;241m=\u001b[39m model(perturbed_data)\n",
      "Cell \u001b[0;32mIn[7], line 25\u001b[0m, in \u001b[0;36ml2_pgd_attack\u001b[0;34m(model, images, labels, epsilon, alpha, iters)\u001b[0m\n\u001b[1;32m     22\u001b[0m ori_images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mdata\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequires_grad\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# Step 1: Load CIFAR10 Dataset\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "test_set = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=1, shuffle=True)\n",
    "\n",
    "# Step 2: Load Pretrained Model (e.g., ResNet18)\n",
    "model = models.resnet18(pretrained=True).to(DEVICE)\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Define L2 Attack (PGD)\n",
    "def l2_pgd_attack(model, images, labels, epsilon, alpha, iters):\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "    ori_images = images.data\n",
    "\n",
    "    for i in range(iters):\n",
    "        images.requires_grad = True\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = F.cross_entropy(outputs, labels)\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Generate perturbations with the gradient\n",
    "        grad = images.grad.data\n",
    "\n",
    "        # Normalize the gradient for L2 attack\n",
    "        grad_norm = torch.norm(grad.view(grad.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
    "        grad = grad / (grad_norm + 1e-8)  # Avoid division by zero\n",
    "\n",
    "        # Update the image with small steps\n",
    "        adv_images = images + alpha * grad\n",
    "\n",
    "        # Clip the perturbation to stay within epsilon L2 norm\n",
    "        perturbation = adv_images - ori_images\n",
    "        perturbation_norm = torch.norm(perturbation.view(perturbation.size(0), -1), dim=1).view(-1, 1, 1, 1)\n",
    "        perturbation = perturbation * torch.min(torch.ones_like(perturbation_norm), epsilon / perturbation_norm)\n",
    "        \n",
    "        # Update adversarial image\n",
    "        images = ori_images + perturbation\n",
    "        images = torch.clamp(images, -1, 1)  # Keep image in valid range\n",
    "        images = images.detach()  # Detach the t\n",
    "    return images\n",
    "\n",
    "# Step 4: Test the L2 PGD Attack\n",
    "def test_l2_attack(model, test_loader, epsilon, alpha, iters):\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "    \n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        \n",
    "        # Generate adversarial example\n",
    "        perturbed_data = l2_pgd_attack(model, data, target, epsilon, alpha, iters)\n",
    "        \n",
    "        # Re-classify the perturbed image\n",
    "        output = model(perturbed_data)\n",
    "        final_pred = output.max(1, keepdim=True)[1]  # Get the index of the max log-probability\n",
    "        \n",
    "        # Check if the adversarial image was classified correctly\n",
    "        if final_pred.item() == target.item():\n",
    "            correct += 1\n",
    "\n",
    "    final_acc = correct / float(len(test_loader))\n",
    "    print(f\"Test Accuracy = {final_acc * 100:.2f}%\")\n",
    "\n",
    "\n",
    "# Step 5: Set attack parameters and run the attack\n",
    "epsilon = 1.0  # Maximum L2 perturbation\n",
    "alpha = 0.01   # Step size for each iteration\n",
    "iters = 40     # Number of iterations\n",
    "\n",
    "test_l2_attack(model, test_loader, epsilon, alpha, iters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
