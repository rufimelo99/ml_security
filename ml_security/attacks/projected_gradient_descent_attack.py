from typing import List, Optional

import torch
import torch.nn.functional as F
from tqdm import tqdm

from ml_security.attacks.fast_gradient_sign_attack import (
    DenormalizingTransformation,
    FastGradientSignAttack,
)


class ProjectedGradientDescent(FastGradientSignAttack):
    """
    Projected Gradient Descent (PGD) Attack.

    The Projected Gradient Descent (PGD) Attack is a more powerful variant of
    the Fast Gradient Sign Attack (FGSM). Introduced by Madry et al. in "Towards
    Deep Learning Models Resistant to Adversarial Attacks," PGD iteratively applies
    FGSM to the input data, with a small step size, for multiple iterations. This
    iterative approach allows PGD to generate stronger adversarial examples than
    FGSM, making it a popular choice for evaluating the robustness of neural networks.
    """

    def __init__(
        self,
        epsilon: float,
        device: torch.device,
        num_steps: int,
    ) -> None:
        """
        Initializes the Projected Gradient Descent Attack.

        Args:
            epsilon (float): The epsilon value to use for the attack.
            device (torch.device): The device to use for the attack.
            num_steps (int): The number of steps to use for the attack.
        """
        super().__init__(epsilon=epsilon, device=device)
        self.num_steps = num_steps

    def attack(
        self,
        model: torch.nn.Module,
        dataloader: torch.utils.data.DataLoader,
        denormlizing_transform: Optional[DenormalizingTransformation] = None,
    ) -> List:
        """
        Performs the Projected Gradient Descent Attack on the model.

        Args:
            model (torch.nn.Module): The model to attack.
            dataloader (torch.utils.data.DataLoader): The dataloader for the input data.
            denormlizing_transform (Optional[DenormalizingTransformation]): The denormalizing
                transformation to apply to the input data.

        Returns:
            List: The list of adversarial examples generated by the attack.
        """
        model.eval()
        adversarial_examples = []

        for inputs, labels in tqdm(dataloader, desc="Attacking"):
            inputs, labels = inputs.to(self.device), labels.to(self.device)

            if denormlizing_transform:
                inputs = self._denorm(
                    inputs, denormlizing_transform.mean, denormlizing_transform.std
                )

            # Generate adversarial examples using PGD
            perturbed_inputs = self._pgd_attack(model, inputs, labels)

            adversarial_examples.append(perturbed_inputs)

        return adversarial_examples, adversarial_examples

    def _pgd_attack(
        self,
        model: torch.nn.Module,
        inputs: torch.Tensor,
        labels: torch.Tensor,
    ) -> torch.Tensor:
        """
        Performs the Projected Gradient Descent (PGD) Attack on the input data.

        Args:
            model (torch.nn.Module): The model to attack.
            inputs (torch.Tensor): The input data to attack.
            labels (torch.Tensor): The true labels for the input data.

        Returns:
            torch.Tensor: The perturbed input data.
        """
        # Initialize the perturbed inputs as the original inputs
        perturbed_inputs = inputs.clone().detach()

        # Set requires_grad attribute of tensor
        perturbed_inputs.requires_grad = True

        # Perform PGD Attack
        for _ in range(self.num_steps):
            # Forward pass the perturbed inputs through the model
            outputs = model(perturbed_inputs)

            # Calculate the loss
            loss = F.nll_loss(outputs, labels)

            # Zero the gradients
            model.zero_grad()

            # Backward pass to calculate the gradients
            loss.backward()

            # Collect the data gradients
            data_grad = perturbed_inputs.grad.data

            # Perform the FGSM update
            perturbed_inputs = self._fgsm_attack(
                perturbed_inputs, self.epsilon, data_grad
            )

        return perturbed_inputs
